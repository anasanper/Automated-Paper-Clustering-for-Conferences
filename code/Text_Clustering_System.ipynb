{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c78052ec",
   "metadata": {},
   "source": [
    "#  Functions for GitHub Repository: Automated-Paper-Clustering-for-Conferences\n",
    "\n",
    "This notebook contains code for extracting, preprocessing, clustering, and visualizing papers from conference proceedings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0c6af5",
   "metadata": {},
   "source": [
    "<pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502fa5eb",
   "metadata": {},
   "source": [
    "## Function to Write Texts Extracted from Papers\n",
    "\n",
    "This function extracts text from PDF files and writes them into text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a67d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import mkdir\n",
    "from pdfminer.high_level import extract_text  # Ensure pdfminer is installed\n",
    "\n",
    "# Function to write texts extracted from papers into text files\n",
    "def write_texts(papers, folder_papers, folder_texts):\n",
    "    problem_files = []  # List to store names of problematic files\n",
    "    mkdir(folder_texts)  # Create folder to store text files\n",
    "    \n",
    "    for paper in papers:\n",
    "        try:\n",
    "            os.chdir(folder_papers)  # Switch to the directory where PDF documents are located\n",
    "            text = extract_text(paper)  # Use pdfminer to extract text from the PDF\n",
    "            file_name = paper + '.txt'\n",
    "            \n",
    "            os.chdir(folder_texts)  # Switch to the directory where text files will be stored\n",
    "            with open(file_name, 'w', encoding='utf-8') as file:\n",
    "                file.write(text)\n",
    "                file.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {paper}: {str(e)}\")\n",
    "            problem_files.append(paper)  # Add problematic file to the list of files with issues\n",
    "    \n",
    "    return problem_files  # Return the list of problematic files at the end of the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75496e78",
   "metadata": {},
   "source": [
    "## Function to Extract Paper Titles\n",
    "\n",
    "\n",
    "This function extracts paper titles from text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45679e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract paper titles from text files in a specified folder.\n",
    "def papers_names(folder_texts):\n",
    "    \n",
    "    files_names = [] # Initialize an empty list to store the titles of the papers\n",
    "    files = os.listdir(folder_texts) # List all files in the specified folder\n",
    "\n",
    "    # Iterate through each file in the folder\n",
    "    for file_name in files:\n",
    "        with open(os.path.join(folder_texts, file_name), 'r', encoding='utf-8') as file:\n",
    "            text = file.read()    \n",
    "            title_end_index = text.find(\"\\n\\n\") # Find the index of the first occurrence of \"\\n\\n\" \n",
    "                                                # which indicates the end of the title\n",
    "            title = text[:title_end_index].replace(\"\\n\", \"\").strip() # Extract the title from the\n",
    "            # remove any newline characters, and strip any leading or trailing whitespace\n",
    "            \n",
    "            # Add the extracted title to the list of paper titles\n",
    "            files_names.append(title)\n",
    "    \n",
    "    return files_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b75cd8",
   "metadata": {},
   "source": [
    "## Text Preprocessing Functions\n",
    "\n",
    "\n",
    "These functions preprocess the extracted text by removing noise and applying lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5dc1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "# Returns False for words with characters that are not letters from the alphabet a-z\n",
    "\n",
    "def abc(w):\n",
    "    x = True\n",
    "    for char in w:\n",
    "        if ord(char) not in range(97, 123):\n",
    "            x = False\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from string import digits\n",
    "\n",
    "# Text preprocessing: removes (cid:nÂº), 'fig' and applies abc\n",
    "# The lines of the function are in this order to be able to remove 'cid' \n",
    "#even though transformers remove numbers, punctuation, and uppercase letters\n",
    "\n",
    "def prep_text(text):\n",
    "    text = text.lower()  # Convert all text to lowercase\n",
    "    text = text.translate(str.maketrans('', '', digits))  # Remove numbers\n",
    "    text = text.replace('(cid:)', '')  # 'CID' is an identifier\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = ' '.join(word for word in text.split() if word != 'fig' and abc(word) == True)  \n",
    "        # Apply the abc custom function to each word\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9d7d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# nltk.download('all') # Download all NLTK resources only if it's the first time importing NLTK in this environment\n",
    "                       # comment this line otherwise\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Function that maps NLTK POS tags to WordNet POS tags\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Function for lemmatizing text (reducing words to their base form)\n",
    "def lemat(text):\n",
    "    lema = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text)  # Tokenize the text (split all words and add them to a list in order)\n",
    "    tagged_tokens = pos_tag(tokens)  # List of tuples (tuple = (token, tag)), tag: type of word (noun, adjective, etc.)\n",
    "    words_lema = []\n",
    "    for token, tag in tagged_tokens:\n",
    "        words_lema.append(lema.lemmatize(token, pos_tagger(tag)))  # Lemmatize each token indicating its POS tag\n",
    "    text = ' '.join(word for word in words_lema)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df1baa8",
   "metadata": {},
   "source": [
    "## Function to Create Corpus\n",
    "\n",
    "\n",
    "This function preprocesses the text and creates a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b3204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess and create a corpus using either stemming or lemmatization\n",
    "def txt_corpus(folder_texts, root_function):\n",
    "    os.chdir(folder_texts)\n",
    "    corpus = []\n",
    "    files = os.listdir(folder_texts)  # List of files in the specified folder\n",
    "    for file_name in files:\n",
    "        with open(file_name, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            file.close()\n",
    "        text = prep_text(text)  # Apply the prep_text function\n",
    "        text = root_function(text)  # Apply either stemming or lemmatization\n",
    "        corpus.append(text)  # Add the preprocessed paper to the corpus vector\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e13045",
   "metadata": {},
   "source": [
    "## Function to Convert Corpus into TF-IDF Matrix\n",
    "\n",
    "\n",
    "This function converts the corpus into a TF-IDF matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe513ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert the corpus into a TF-IDF matrix\n",
    "# The values in this matrix represent the importance of each word in each text with respect to the corpus\n",
    "\n",
    "def tfidf(corpus, parameters):\n",
    "    transformer = TfidfVectorizer(**parameters)\n",
    "    matrix_tfidf = transformer.fit_transform(corpus)\n",
    "    return matrix_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97567503",
   "metadata": {},
   "source": [
    "## Function to Plot Cosine Similarity Matrix\n",
    "\n",
    "\n",
    "This function calculates and plots the cosine similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94617809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def plot_cosine_similarity(matrix_tfidf):\n",
    "    # Calculate cosine similarity matrix\n",
    "    matrix_sim = cosine_similarity(matrix_tfidf)\n",
    "\n",
    "    # Plot the similarity matrix\n",
    "    f, ax = plt.subplots(figsize=(50, 50))  # Determine the dimensions of the figure\n",
    "    sns.heatmap(matrix_sim, annot=True, linewidths=.5, fmt='.2f', cmap=\"YlGnBu\")\n",
    "    plt.show()\n",
    "\n",
    "    # Return the cosine similarity matrix\n",
    "    return matrix_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf07940",
   "metadata": {},
   "source": [
    "## Function for Elbow Method\n",
    "\n",
    "\n",
    "This function uses KMeans on the similarity matrix and plots Within-Cluster Sum of Squares (WCSS) to find optimal clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c857d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#elbow_method uses KMeans on similarity matrix, plots WCSS to find optimal clusters for given data.\n",
    "\n",
    "def elbow_method(matrix_sim, n_clusters):\n",
    "    wcss = []\n",
    "    cl_num = n_clusters + 1\n",
    "    for i in range(1, cl_num):\n",
    "        kmeans = KMeans(i)\n",
    "        kmeans.fit(matrix_sim)\n",
    "        wcss_iter = kmeans.inertia_\n",
    "        wcss.append(wcss_iter)\n",
    "\n",
    "    number_clusters = range(1, cl_num)\n",
    "    plt.plot(number_clusters, wcss)\n",
    "    plt.title('The Elbow Method')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Within-cluster Sum of Squares')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff0133",
   "metadata": {},
   "source": [
    "## Function for Clustering\n",
    "\n",
    "\n",
    "This function performs clustering and visualizes the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6457e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def clustering(matrix_sim, n_clusters, papers_titles, max_elements_per_cluster, folder_clusters):\n",
    "    \n",
    "    num_papers = len(papers_titles)\n",
    "    \n",
    "    if n_clusters >= num_papers:\n",
    "        n_clusters = num_papers - 1\n",
    "        print(\"Warning: Number of clusters is high. Adjusting to maximum possible value:\", n_clusters)\n",
    "        \n",
    "    if max_elements_per_cluster < (num_papers/ n_clusters):\n",
    "        max_elements_per_cluster = int(num_papers/ n_clusters) + 1\n",
    "        print(\"Warning: Maximum elements per cluster is low. Adjusting to minimum possible value:\", max_elements_per_cluster)\n",
    "\n",
    "    # Execute clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters).fit(matrix_sim)\n",
    "    \n",
    "    # Get labels and centroids\n",
    "    labels = kmeans.labels_\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    \n",
    "    # Dictionary to store indices selected by cluster\n",
    "    selected_indices_by_cluster = {}\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        # Find indices of points assigned to cluster i\n",
    "        cluster_indices = np.where(labels == i)[0]\n",
    "\n",
    "        # Check if there are enough points in the cluster to select\n",
    "        num_points_in_cluster = len(cluster_indices)\n",
    "\n",
    "        if num_points_in_cluster > max_elements_per_cluster:\n",
    "            # Calculate distance from each point to cluster centroid\n",
    "            distances = np.linalg.norm(matrix_sim[cluster_indices] - centroids[i], axis=1)\n",
    "\n",
    "            # Sort indices of points based on distance to centroid\n",
    "            sorted_indices = cluster_indices[np.argsort(distances)]\n",
    "\n",
    "            # Select the first max_elements_per_cluster points and store them\n",
    "            selected_indices = sorted_indices[:max_elements_per_cluster]\n",
    "\n",
    "            # Store selected indices in dictionary\n",
    "            selected_indices_by_cluster[i] = selected_indices\n",
    "        else:\n",
    "            # If there are fewer points than max_elements_per_cluster, select all available points\n",
    "            selected_indices_by_cluster[i] = cluster_indices\n",
    "\n",
    "    # Assign remaining indices to nearest clusters\n",
    "    unassigned_indices = [idx for idx in range(len(matrix_sim)) if idx not in np.concatenate(list(selected_indices_by_cluster.values()))]\n",
    "\n",
    "    for idx in unassigned_indices:\n",
    "        distances = [np.linalg.norm(matrix_sim[idx] - centroids[j]) for j in range(n_clusters)]\n",
    "        # Find the closest cluster that has not reached its capacity limit yet\n",
    "        closest_cluster = np.argmin(distances)\n",
    "        # Look for the next available cluster if the closest cluster is full\n",
    "        for j in range(1, n_clusters):\n",
    "            next_cluster = (closest_cluster + j) % n_clusters\n",
    "            if len(selected_indices_by_cluster[next_cluster]) < max_elements_per_cluster:\n",
    "                selected_indices_by_cluster[next_cluster] = np.append(selected_indices_by_cluster[next_cluster], idx)\n",
    "                break\n",
    "\n",
    "    # Transform the dictionary to a labels array for next steps\n",
    "    values_keys = [(value, key) for key, values in selected_indices_by_cluster.items() for value in values]\n",
    "    labels = [key for value, key in values_keys]\n",
    "    values_keys.sort()\n",
    "    labels = [key for value, key in values_keys]\n",
    "\n",
    "    # Visualize the results\n",
    "    plt.figure(figsize=(5, 5))  # Set the figure size\n",
    "    clusters_visual = plt.scatter(matrix_sim[:, 0], matrix_sim[:, 1], c=labels, s=50, cmap='rainbow')\n",
    "    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='x', s=200, c=range(len(kmeans.cluster_centers_)), cmap='rainbow')\n",
    "    plt.colorbar(clusters_visual)\n",
    "    plt.show()\n",
    "\n",
    "    # Silhouette score to evaluate clustering\n",
    "    silhouette = silhouette_score(matrix_sim, labels)\n",
    "    print('Silhouette score =', silhouette)\n",
    "    \n",
    "     # Generating unique CSV file name based on number of clusters and max elements per cluster\n",
    "    csv_name = f'clusters_papers_{n_clusters}_{max_elements_per_cluster}.csv'\n",
    "   \n",
    "    # Writing cluster assignments to CSV\n",
    "    os.chdir(folder_clusters)\n",
    "    with open(csv_name, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Paper Title', 'Cluster'])\n",
    "        for i, cluster in enumerate(labels):\n",
    "            writer.writerow([papers_titles[i], cluster])\n",
    "            \n",
    "    # Return the plot and silhouette score      \n",
    "    return clusters_visual, silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc62940c",
   "metadata": {},
   "source": [
    "<pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98798ac8",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "\n",
    "\n",
    "The main execution section prompts the user for input paths, performs text extraction, preprocessing, clustering, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd33f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User inputs\n",
    "folder_papers = input(r\"Please provide the path to the papers folder: \")\n",
    "folder_texts = input(r\"Please provide the path to save the extracted texts: \")\n",
    "folder_clusters = input(r\"Please provide the path to save the clusters of paper titles:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fba44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The letter 'r' is added to the beginning of the paths entered by the user, ensuring that they are treated as raw strings\n",
    "folder_papers = r\"\" + folder_papers\n",
    "folder_texts = r\"\" + folder_texts\n",
    "\n",
    "# Extract texts from papers and write them to text files\n",
    "# Notify about problematic PDFs\n",
    "papers = os.listdir(folder_papers)\n",
    "problems = write_texts(papers, folder_papers, folder_texts)\n",
    "if problems:\n",
    "    print(\"Problematic files:\")\n",
    "    for problem in problems:\n",
    "        print(problem)\n",
    "        \n",
    "num_pdfs = len(papers)\n",
    "num_texts = len(papers) - len(problems)\n",
    "num_problems = len(problems)\n",
    "print(f\"The number of PDFs is {num_pdfs}, the number of extracted texts is {num_texts}, and the number of problematic PDFs is {num_problems}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52eaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the papers titles\n",
    "papers_titles = papers_names(folder_texts)\n",
    "\n",
    "# Create a corpus from the extracted texts\n",
    "corpus = txt_corpus(folder_texts, lemat)\n",
    "\n",
    "# Transform the corpus using TF-IDF vectorizer\n",
    "parameters = {'max_df' : 0.7, 'min_df' : 0.2, 'stop_words' : 'english', 'max_features' : 50, 'sublinear_tf' : True}\n",
    "matrix_tfidf = tfidf(corpus, parameters)\n",
    "\n",
    "# Calculate and plot cosine similarity matrix\n",
    "matrix_sim = plot_cosine_similarity(matrix_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a95d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for the number of clusters\n",
    "n_clusters = int(input(\"Please enter the desired number of clusters: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27c55ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#elbow_method uses KMeans on similarity matrix, plots WCSS to find optimal clusters for given data.\n",
    "elbow_method(matrix_sim, n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4349bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir(folder_clusters) # Create folder to store cluster files\n",
    "\n",
    "while True:\n",
    "    # Input for the number of clusters\n",
    "    n_clusters = int(input(\"Please enter the desired number of clusters: \"))\n",
    "\n",
    "    # Input for the maximum number of elements per cluster\n",
    "    max_elements_per_cluster = int(input(\"Please enter the maximum number of elements per cluster: \"))\n",
    "\n",
    "    clusters_visual, silhouette= clustering(matrix_sim, n_clusters, papers_titles, max_elements_per_cluster, folder_clusters)\n",
    "\n",
    "    # Ask the user if they want to try another number of clusters and maximum number of elements.\n",
    "    another_try = input(\"Do you want to try another number of clusters and maximum number of elements? (y/n): \").lower()\n",
    "    \n",
    "    if another_try != 'y':\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
